{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/dateappdata/finalized_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将字符串形式的向量转换为实际的列表形式\n",
    "import ast\n",
    "df['numeric_doc2vec_merged_vector'] = df['numeric_doc2vec_merged_vector'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'About Me': 652.6228366615463\n",
      "Average words estimate (approximately 7 characters per word): 93.23183380879233\n"
     ]
    }
   ],
   "source": [
    "def calculate_length(text):\n",
    "    return len(text)\n",
    "\n",
    "df.dropna(subset=['About Me'], inplace=True)\n",
    "\n",
    "# 应用函数计算每个 \"About Me\" 文本的长度，并创建一个新的列\n",
    "df['about_me_length'] = df['About Me'].apply(calculate_length)\n",
    "\n",
    "# 计算 \"About Me\" 列的平均长度，跳过空值\n",
    "average_length = df['about_me_length'].mean(skipna=True)\n",
    "\n",
    "# 打印平均长度\n",
    "print(\"Average length of 'About Me':\", average_length)\n",
    "average_words_estimate = average_length / 7\n",
    "print(\"Average words estimate (approximately 7 characters per word):\", average_words_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m204.8/227.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Installing collected packages: h11, httpcore, httpx, openai\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# 设置你的 OpenAI API 密钥\n",
    "openai.api_key = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 DataFrame 中随机抽取 10 个样本\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['income'] != -1]\n",
    "df = df[df['income'] != '-1']\n",
    "sample_df = df.sample(n=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10283     80000\n",
      "1743      20000\n",
      "13531     20000\n",
      "19926     40000\n",
      "20434    100000\n",
      "2859      80000\n",
      "26241     50000\n",
      "11655     20000\n",
      "26805     20000\n",
      "16459     60000\n",
      "Name: income, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sample_df['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=openai.api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 'About Me' texts are saved in 'about_me_texts_religion_updated.txt'\n"
     ]
    }
   ],
   "source": [
    "# 定义一个生成 \"About Me\" 的函数\n",
    "\n",
    "def generate_about_me(religion_clean, age, sex, orientation, drinks, drugs, education, ethnicity, income, job, smokes):\n",
    "    prompt = f\"now you are a person that Religion: {religion_clean}\\nAge: {age}\\nSex: {sex}\\nOrientation: {orientation}\\n Drinks: {drinks}\\nDrugs: {drugs}\\nEducation: {education}\\nEthnicity: {ethnicity}\\n Income: {income}\\nJob: {job}\\n Smokes: {smokes}\\n\\nGenerate an 'About Me' text for a dating app, about 100 words Please do not repeated those characteristics. Since others already know. Just imagine if you are this person what would you say about you. About 100 words\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response =  client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 创建一个文件来存储生成的文本\n",
    "file_path = \"about_me_texts_religion_updated.txt\"\n",
    "\n",
    "# 对每个样本生成 \"About Me\" 文本，并写入文件\n",
    "with open(file_path, \"w\") as file:\n",
    "    for index, row in sample_df.iterrows():\n",
    "        religion_clean = row['religion_clean']\n",
    "        age = row['age']\n",
    "        sex = row['sex']\n",
    "        orientation = row['orientation']\n",
    "        drinks = row['drinks']\n",
    "        drugs = row['drugs']\n",
    "        education = row['education']\n",
    "        ethnicity = row['ethnicity']\n",
    "        income = row['income']\n",
    "        job = row['job']\n",
    "        smokes = row['smokes']\n",
    "\n",
    "        about_me_text = generate_about_me(religion_clean, age, sex, orientation, drinks, drugs, education, ethnicity, income, job, smokes)\n",
    "        file.write(f\"Sample {index + 1}:\\n\")\n",
    "        file.write(about_me_text)\n",
    "        file.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"Generated 'About Me' texts are saved in '{file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 'About Me' texts are saved in 'about_me_texts_updated.txt'\n"
     ]
    }
   ],
   "source": [
    "# 定义一个生成 \"About Me\" 的函数\n",
    "\n",
    "def generate_about_me(age, sex, orientation, drinks, drugs, education, ethnicity, income, job, smokes):\n",
    "    prompt = f\"now you are a person that Age: {age}\\nSex: {sex}\\nOrientation: {orientation}\\n Drinks: {drinks}\\nDrugs: {drugs}\\nEducation: {education}\\nEthnicity: {ethnicity}\\n Income: {income}\\nJob: {job}\\n Smokes: {smokes}\\n\\nGenerate an 'About Me' text for a dating app, about 100 words Please do not repeated those characteristics. Since others already know. Just imagine if you are this person what would you say about you. About 100 words\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response =  client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 创建一个文件来存储生成的文本\n",
    "file_path = \"about_me_texts_updated.txt\"\n",
    "\n",
    "# 对每个样本生成 \"About Me\" 文本，并写入文件\n",
    "with open(file_path, \"w\") as file:\n",
    "    for index, row in sample_df.iterrows():\n",
    "        age = row['age']\n",
    "        sex = row['sex']\n",
    "        orientation = row['orientation']\n",
    "        drinks = row['drinks']\n",
    "        drugs = row['drugs']\n",
    "        education = row['education']\n",
    "        ethnicity = row['ethnicity']\n",
    "        income = row['income']\n",
    "        job = row['job']\n",
    "        smokes = row['smokes']\n",
    "\n",
    "        about_me_text = generate_about_me(age, sex, orientation, drinks, drugs, education, ethnicity, income, job, smokes)\n",
    "        file.write(f\"Sample {index + 1}:\\n\")\n",
    "        file.write(about_me_text)\n",
    "        file.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"Generated 'About Me' texts are saved in '{file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 'About Me' texts are saved in 'message_me_texts_religion_updated.txt'\n"
     ]
    }
   ],
   "source": [
    "# 定义一个生成 \"About Me\" 的函数\n",
    "\n",
    "def generate_message_me(religion_clean, age, sex, orientation, drinks, drugs, education, ethnicity, income, job, smokes):\n",
    "    prompt = f\"now you are a person that Religion: {religion_clean}\\nAge: {age}\\nSex: {sex}\\nOrientation: {orientation}\\n Drinks: {drinks}\\nDrugs: {drugs}\\nEducation: {education}\\nEthnicity: {ethnicity}\\n Income: {income}\\nJob: {job}\\n Smokes: {smokes}\\n\\nGenerate an 'Message me if' text for a dating app, about 100 words Please do not repeated those characteristics. Since others already know. Just imagine if you are this person what would you say about you. About 100 words\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response =  client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 创建一个文件来存储生成的文本\n",
    "file_path = \"message_me_texts_religion_updated.txt\"\n",
    "\n",
    "# 对每个样本生成 \"About Me\" 文本，并写入文件\n",
    "with open(file_path, \"w\") as file:\n",
    "    for index, row in sample_df.iterrows():\n",
    "        religion_clean = row['religion_clean']\n",
    "        age = row['age']\n",
    "        sex = row['sex']\n",
    "        orientation = row['orientation']\n",
    "        drinks = row['drinks']\n",
    "        drugs = row['drugs']\n",
    "        education = row['education']\n",
    "        ethnicity = row['ethnicity']\n",
    "        income = row['income']\n",
    "        job = row['job']\n",
    "        smokes = row['smokes']\n",
    "\n",
    "        message_me_text = generate_message_me(religion_clean, age, sex, orientation, drinks, drugs, education, ethnicity, income, job, smokes)\n",
    "        file.write(f\"Sample {index + 1}:\\n\")\n",
    "        file.write(message_me_text)\n",
    "        file.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"Generated 'Message Me' texts are saved in '{file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 'Message Me' texts are saved in 'message_me_texts_updated.txt'\n"
     ]
    }
   ],
   "source": [
    "# 定义一个生成 \"About Me\" 的函数\n",
    "\n",
    "def generate_message_me(age, sex, orientation, drinks, drugs, education, ethnicity, income, job, smokes):\n",
    "    prompt = f\"now you are a person that Age: {age}\\nSex: {sex}\\nOrientation: {orientation}\\n Drinks: {drinks}\\nDrugs: {drugs}\\nEducation: {education}\\nEthnicity: {ethnicity}\\n Income: {income}\\nJob: {job}\\n Smokes: {smokes}\\n\\nGenerate an 'Message me if' text for a dating app, about 100 words Please do not repeated those characteristics. Since others already know. Just imagine if you are this person what would you say about you. About 100 words\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response =  client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 创建一个文件来存储生成的文本\n",
    "file_path = \"message_me_texts_updated.txt\"\n",
    "\n",
    "# 对每个样本生成 \"About Me\" 文本，并写入文件\n",
    "with open(file_path, \"w\") as file:\n",
    "    for index, row in sample_df.iterrows():\n",
    "        age = row['age']\n",
    "        sex = row['sex']\n",
    "        orientation = row['orientation']\n",
    "        drinks = row['drinks']\n",
    "        drugs = row['drugs']\n",
    "        education = row['education']\n",
    "        ethnicity = row['ethnicity']\n",
    "        income = row['income']\n",
    "        job = row['job']\n",
    "        smokes = row['smokes']\n",
    "\n",
    "        message_me_text = generate_message_me(age, sex, orientation, drinks, drugs, education, ethnicity, income, job, smokes)\n",
    "        file.write(f\"Sample {index + 1}:\\n\")\n",
    "        file.write(message_me_text)\n",
    "        file.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"Generated 'Message Me' texts are saved in '{file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[['About Me', 'Ideal Type']].to_csv('about_ideal.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
